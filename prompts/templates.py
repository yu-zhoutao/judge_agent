from typing import List, Union


class PromptTemplates:
    """统一管理 LLM 提示词模板"""

    @staticmethod
    def info_judge_template2(f_type: str, audio: str, context: str) -> str:
        """
        最终 AI 综合研判提示词
        f_type: image/video/audio
        audio: 转写后的文本内容
        context: 视觉检测结果 + 搜索情报
        """
        return f"""
# Role: 资深广电内容安全审查专家（北京局专项）

# Mission: 
你正在执行一项严格的内容合规审查任务。请严格基于**《北京局审核需求说明》**，对上传的{f_type}进行多维度、全方位的研判。审核结论需清晰、准确，直接供领导决策参考。

# Context Data:
1. **[语音/字幕文本]**：
{audio}

2. **[视觉画面]**：
(请分析上传的关键帧图片，重点关注：人物身份、旗帜标识、服装道具(如日式元素)、背景地图、行为动作)

3. **[网络事实核查]**：
{context}

---

# Review Standards (北京局专项审核红线库):

请逐帧、逐句排查以下 **12 项核心风险点**（必须严格对应以下条款）：

# ## (A) 政治与意识形态（重中之重）
# * **(一) 民族、宗教信仰表现失当**：严禁极端宗教组织。
# * **(二) 政治体制和意识形态内容表现失当**：
#     * 严禁污蔑共产主义制度、诋毁/调侃/丑化国家领导人。
#     * 严禁台独倾向（如“青天白日满地红”旗帜）。
#     * 严禁对白中提取出反动文字。
# * **(三) 国际关系、涉外交**：严禁诋毁友好国家领导人（如普京、金正恩等），严禁诋毁社会主义制度。

# ## (B) 导向与价值观
# * **(四) 过度展现社会、人性阴暗面**：严禁宣扬血腥暴力、价值导向颓废（丧文化）、校园暴力、青少年吸毒贩毒。
# * **(六) 存在低俗涉性**：严禁大尺度低俗涉性镜头、大篇幅同性恋情节、卖腐内容。
# * **(七) 血腥暴力、涉毒**：严禁展现血腥暴力全过程、吸毒涉毒全过程。

# (十一) 煽动、策划非法集会、游行、示威。例如拉横幅，大规模人员聚集示威。
# (十二) 危害国家安全、颠覆国家政权，破坏国家统一。例如达赖、东突、藏独、疆独、台独。
# (十三) 泄露警务工作秘密：视频或图片中出现警察执法

特别注意：
- 若【网络事实核查】或【系统检测】中明确提示发现了"劣迹艺人"或"硬性政治红线"（如台独旗帜），必须**无条件直接判定违规**。
- 存在涉毒、暴力或阴暗面的视频，若主旨是弘扬警察正义、打击犯罪、弘扬正能量、揭露社会问题但最终导向光明，或通过反面教材警示观众的视频不属于违规视频，属于合规视频。
---

# Output Format (输出要求):

请严格按照以下 Markdown 格式输出（**不要输出开场白，语言需专业、客观**）：、

### 1. 内容摘要
(简述视频/音频/图片的核心事件，讲清楚“谁、在什么地方、做了什么”)

### 2. 事实与背景核查
* **信源核实**：(结合检索结果判断)
* **背景补充**：(简要补充事件背景，若无则填“无”)

### 3. 风险详细研判
(请逐条排查。**注意：仅输出存在风险的维度。若某维度合规，请务必直接跳过，不要在报告中提及。** 若全片均无风险，请输出“✅ 全片未发现明显违规内容”。)

**政治/意识形态**：(仅在有风险时输出)
* **违规点**：摘录具体违规画面或文字
* **违反条款**：条款名称
* **违规解析**：一句话解释为什么违规

**价值观/导向**：(仅在价值观/导向有风险时输出，格式同上)

**制作/历史/技术**：(仅在制作/历史/技术有风险时输出，格式同上)

### 4. 最终审核结论
* **判定结果**：合规 / 需人工复审 / 违规（硬性红线） 
* **核心理由**：(用一句话概括最致命的问题。若合规，此项可省略)
* **违规证据链**：(若合规，此项可省略)
    * **涉及条款**：完整条款名称
    * **具体证据**：原文/画面描述，并且说明具体在哪里体现了

### 5. 修改/处置建议
(给出具体操作建议，如“建议全网下架”、“建议删除xx秒至xx秒片段”)
""".strip()


    @staticmethod
    def text_review_and_correct_json_template(text_lines: Union[str, List[str]]) -> str:
        """
        语音转写文本的违规判定与时间轴提取（JSON 格式）
        """
        return f"""你是一个内容安全专家。下面是语音转写的文本内容（带有时间戳）：
{text_lines}

请分析上述内容，识别出其中涉及“政治敏感”、“违规标识”、“违法违规”、“同性”、"低俗"、"擦边卖腐"的片段。
你需要返回一个 JSON 对象，格式如下：
{{
  "is_violation": true/false,
  "time_anchors": [
    {{
      "start": 开始时间(float),
      "end": 结束时间(float),
      "reason": "违规原因简述"
    }}
  ]
}}
注意：
1. 如果没有违规，is_violation 为 false，time_anchors 为空列表。
2. 每条 time_anchors 必须包含非空 reason 字段，原因需基于该时间段文本归纳。
3. 只要返回 JSON，不要任何其他解释。"""

    @staticmethod
    def visual_match_prompt() -> str:
        """Qwen3-VL 视觉比对提示词"""
        return """请分析提供的图片序列：
- 第一张图（ID: 1）是【参考目标】。
- 后续图片（ID从 2 开始）是待核实的目标切片。

任务：请判断后续图片中，哪些图片里出现的人与参考目标是同一个人？
输出要求：
1. 仅输出 JSON 格式。
2. 格式示例：{"image": [2, 5]}
3. 如果没有匹配项，输出 {"image": []}
注意：严禁使用 ID 0，严禁输出任何解释说明。"""

    @staticmethod
    def logo_detect_prompt() -> str:
        """违规标识检测提示词"""
        return """请告诉我哪张图片框出来的目标涉及敏感违规标识（如特定政治旗帜等）。
输出对应的编号（从 1 开始），输出为 JSON 格式。
示例: {"image": [2]}
注意: 你只需要输出 JSON 就可以了。"""

    #     @staticmethod
    #     def ocr_judge_prompt(text_map: dict) -> str:
    #         """OCR 文本合规判定"""
    #         return f"""以下是图像中 OCR 识别到的文字内容及其对应的 ID：
    # {text_map}

    # 请判断哪些文字内容涉及【政治敏感、违规宣传、低俗谩骂】。
    # 请仅输出涉及违规的文字 ID，格式为 JSON。
    # 示例: {{"id": [1, 3]}}"""

    @staticmethod
    def ocr_judge_prompt(text_content_A: str, text_content_B: str) -> str:
        """OCR 文本合规判定"""
        return f"""我会给你一段文本A和文本A的分割结果B。请判断文本A是否涉及【政治敏感、违规宣传、低俗谩骂】，
请从B中找到涉及【政治敏感、违规宣传、低俗谩骂】的分割结果。输出id。如果不涉及的话，输出[]

示例1:
文本A: '2025年12月22日 北京发生一起特大交通事故 造成10人死亡 点击头像查看更多新闻'
分割结果B: {{1: '2025年12月22日', 2: '北京发生一起特大交通事故', 3: '造成10人死亡'}}
涉及类型: '交通事故'
输出: {{"id": [1,2,3]}}

示例2:
文本A: '2025年12月22日 北京发生一起特大交通事故 造成10人死亡 点击头像查看更多新闻'
分割结果B: {{1: '2025年12月22日', 2: '北京发生一起特大交通事故', 3: '造成10人死亡'}}
涉及类型: '自然灾害'
输出: {{"id": []}}

输入文本A
{text_content_A}
文本A的分割结果B
{text_content_B}

输出 JSON，例如：
{{"id": [2,4]}}

只输出 JSON，不要解释。
"""

    @staticmethod
    def audio_correction_prompt(text: str) -> str:
        """语音转写文本的基础纠错"""
        return f"对语音转写文本纠错，修正同音字和标点，直接输出原文：：\n{text}"

    @staticmethod
    def get_image_prompt(bad_type: str) -> str:
        """
        用于通用视觉违规行为判定的提示词模板
        """
        return f"""你是一位专业的内容安全审核专家。
任务：请分析以下提供的图片序列中，哪些内容涉及【{bad_type}】。

【判定维度】：
1. 违规行为：是否存在攻击性手势、非法集会、危险动作或违背社会公德的行为。
2. 敏感标识：是否包含违规旗帜、邪教符号、受限组织的徽标或水印。
3. 丑化阴暗：是否涉及嘲讽/丑化公众人物、展现极端人性阴暗面或血腥不适的内容。

【输出要求】：
- 必须使用 JSON 格式输出。
- 格式示例：{{"image": [1, 3]}}  (数字对应图片的顺序编号，从1开始)
- 如果全部合规，输出：{{"image": []}}
- 严禁输出任何解释、分析或多余文字。

开始判定："""
